---
title: "Heterogeneity"
format: pdf
editor: visual
---

## Introduction to variation

The goal of this document is to get intimate with variation and variance. This is because we will then move on to understanding heterogeneity and to do that we need to be on very solid grounds regarding variance, since they are literally synonymous in meta-analysis. Let's start with the very basics and then move on to more complex calculations.

Let's start by creating an entire population.

```{r}
population <- c(2,5,1,5,7,3,7,1)
```

This is our population. Variance is the average squared distances from the mean of the population, that is, the sum of the squared residuals divided by the population size. Let's calculate the variance of our population using the formula:

$$
\sigma^2 = \frac{\sum x- \mu}N
$$

```{r}
pop_var <- (sum((mean(population)-population)^2))/(length(population))
pop_var
```

Nice, now consider that we take a sample from our population, the sample variance will be different than the variance within the population.

```{r}
set.seed(6539)
our_sample <- sample(population, 5, replace = F)
```

We use the same formula but include Bessel's correction by subtracting 1 from the size of our sample.

$$
sd^2 = \frac{\sum x- \mu}{n-1}
$$

```{r}
(sum((mean(our_sample)-our_sample)^2))/(length(our_sample)-1)
```

Notice how the current estimated variance is much lower than the true variance in the population. This is because the population is small and somewhat diverse. The variation/preposition with which we estimate the mean is known as the standard error, or the standard deviation of the sampling distribution. Our accuracy will increase as our sample size increases. But when we only have a small population, our sample can only be so big. Let's do the same thing we did previously but on a larger scale.

```{r}
new_population <- rnorm(mean = mean(population), 
                        sd   = sqrt(pop_var),
                        n    = 1e5)
mean(population)
```

Let's take a random sample of 30 observations from the population. And compare the sampled mean from the true mean.

```{r}
new_sample <- sample(new_population, size = 30, replace = F)
mean(new_sample)
```

Pretty close, we can also estimate the standard error to get an approximation of the precision of our mean.

```{r}
sd(new_sample)/sqrt(length(new_sample))
```

If we take our estimate =/- the standard error we can get a pretty good picture of the population mean. Now let's estimate the variation again.

```{r}
new_pop_var <- (sum((mean(new_population)-new_population)^2))/(length(new_population))
new_pop_var
```

Let's compare it to our estimation of the variance from our sample, let's use the var() function for some brevity.

```{r}
var(new_sample)
```

Our estimated variation(average squared distance from the mean) is smaller than that of the population. Larger samples will provide more accurate estimations. Let's finish of by taking a larger sample, say n = 100.

```{r}
set.seed(675)
big_sample <- sample(new_population, size = 100, replace = F)

mean(big_sample)
mean(new_population)
```

Quite close, let's compare variation as well.

```{r}
var(big_sample)
new_pop_var

sd(big_sample)
sqrt(new_pop_var)
```

Yup, very close. Let's move on to a more complex problem - meta analysis.

## Meta-analysis

A fair assumption in most meta-analyses is that the true population effect of some stimuli actually consists of a subgroup of true effects that together constitute an overall true effect. That is, the population effect has variation, or heterogeneity. This variability can stem from many sources but for the moment we will not concern ourselves with *why* heterogeneity is present. Note also that we move from having a complete population, to having a population that is studies, meaning that the population is generated from an unobservable hyper-parameter. This makes it so that we never can have a definite answer - however, that is almost never the case anyways. Our data points are now a number of studies containing an effect size and a variation. Let's generate them ourselves.

```{r}
set.seed(673)
k_studies <- 10

es <- rnorm(k_studies, 1, 1)
sigma <- runif(k_studies, .3, 1)
```

The next step is to calculate the inverse variance weights. We do this to give more prominence/weight to studies with less variation.

```{r warning=FALSE}
weights <- 1/sigma
```

Next we calculate a pooled effect size using our observed effects and our variance dependent weights. we do this by dividing the sum of the weighted effects with the sum of the weights - making it ''unbiased''.

```{r}
pooled_effect <- sum(weights*es)/sum(weights)
```

Next we can calculate the heterogeneity, the believe the simplest way is with DerSimonian and Laird's functions. We do this by computing a Q statistic by taking the sum of the weights times the squared difference between our observed effects and our pooled effect. Then we subtract Q by our number of studies to then divide it by the sum of our weights to get tau\^2, for tau we simply take the square root of our calculation.

$$
\tau^2_{DL}=max(0, \frac{ Q-(n-1)}{S_1- S_2/S_1}) 
$$

I believe that the denominator here is simply the sum of the weights, but i could be wrong. That is, the S denominator is the gained using the following formula:

$$
S_r= \sum^n_{i=1}w^r_i
$$

Higgins and Thompson gives the following formula for $\hat\tau^2_{DL}$:

$$
\hat\tau^2_{DL}=\frac{ Q-(k-1)}{\sum w_i- \sum w^2_i/\sum w_i}
$$

```{r}
Q <- sum(weights * (es - pooled_effect)^2)
tau <- (Q - (k_studies-1))/sum(weights)

tau <- sqrt((Q - (k_studies-1))/sum(weights))
tau_denom <-sum(weights) - ((sum(weights^2)) / sum(weights))
tau_higgins <- (Q - (k_studies-1))/tau_denom

```

Great, we now have an estimation of the variation in the effect. We have large variation here, though given that the sigma we specified ranged from 1 to 3 with a true effect size of N(1, 1). Now we only need to check if we are correct, that can easily be done using the metafor package.

```{r message=FALSE}
library(metafor)

tau_2 <- rma.uni(yi = es, vi = sigma, method = 'DL',
                 weights = weights)

sqrt(tau_2$tau2)
```

hmm, neither are the same. I was pretty sure it would match up. It could be due to rounding, but that seems like a stretch. My calculation is probably wrong - must have missed something. It was pretty hard to get tau from the original paper but the silver lining is that the Q is the same in both estimates. Let's do a little simulation so see how the hand calculation and the metafor calculation relates to each other, it could shed some light about what is potentially wrong in my formula.

```{r warning=FALSE}
tau_data <- data.frame(hand_calc = NA, metafor_calc = NA, 
                       hand_q = NA, meta_q = NA )

set.seed(679)
k_studies <- 10
true_es <- .5
sigma <- runif(k_studies, .3, 1)
nsim <- 1000

for (i in 1:nsim) {
  es <- rnorm(k_studies, true_es, 1)
  sigma <- runif(k_studies, .3, 1)
  weights <- 1/sigma
  pooled_effect <- sum(weights*es)/sum(weights)
  hand_q <- sum(weights * (es - pooled_effect)^2)
    
  hand_calc <- sqrt((hand_q - (k_studies-1))/sum(weights))
    
  tau_2 <- rma.uni(yi = es, vi = sigma, method = 'DL',
                 weights = weights)
  metafor_calc <- sqrt(tau_2$tau2)
  meta_q <- tau_2$QE

  tau_data[i,] <- c(hand_calc, metafor_calc, hand_q, meta_q)
  
}

```

Let's plot this out and replace NaN with 0.

```{r}
library(tidyverse)
tau_data$hand_calc[is.nan(tau_data$hand_calc)] <- 0

tau_data %>% 
  ggplot(aes(x = hand_calc, y = metafor_calc))+
  geom_point()+
  geom_smooth()

dif <- tau_data$metafor_calc- tau_data$hand_calc

ggplot()+
  geom_density(aes(dif))


ggplot()+
  geom_histogram(aes(dif), bins = 30)

mean(dif)
```

I am not sure exactly what is going on here, the difference varies but the Q statistic is the same. I feel as if it was only due to me calculating it wrongly the difference should be constant given that the data used is the same. Before looking at if this changes depending on the sample size, let's look at the Q statistic. It should follow a $\chi^2$ distribution with df = k-1. Deviation from this distribution reflects sampling variation beyond what would be expected randomly.

```{r}
set.seed(92895)
chi2 <- rchisq(1000, 9)
ggplot()+
  geom_density(aes(tau_data$meta_q), col = 'blue', size = 1)+
  geom_density(aes(chi2), size = 1)+
  xlab(label = 'Q')
```

The best way to illustrate this is not with random values, but it is nice to see that we get the expected relationship between the expected values and the observed values on just 1000 random Q and $\chi^2$ statistics. Since our observed Qs (blue density) is more right skewed than the random $\chi^2$ statistics(black density) we have evidence that we observe more variation in our effects than to be expected if no heterogeneity is in the sample was present.

```{r warning=FALSE}
tau_data_2 <- data.frame(hand_calc = NA, metafor_calc = NA,
                         true_es=NA)


set.seed(666)
es_range <- seq(0, 1, 0.1)
nsim <- 100
k_studies <- 10

for (i in 1:length(es_range)) {
  for (j in 1:nsim) {
    
    true_es <- es_range[i]
    es <- rnorm(k_studies, es_range[i], 1)
    sigma <- runif(k_studies, .3, 1)
    weights <- 1/sigma
    pooled_effect <- sum(weights*es)/sum(weights)
    Q <- sum(weights * (es - pooled_effect)^2)
    
    hand_calc <- sqrt((Q - (k_studies-1))/sum(weights))
    
    tau_2 <- rma.uni(yi = es, vi = sigma, method = 'DL',
                 weights = weights)
    metafor_calc <- sqrt(tau_2$tau2)

    
    index <- (i - 1) * nsim + j
    tau_data_2[index, ] <- c(hand_calc, metafor_calc, true_es)
  }
  
}
```

Cool, lets plot it out and set the NaN to 0.

```{r}
tau_data_2$hand_calc[is.nan(tau_data_2$hand_calc)] <- 0

tau_data_2 %>% 
  ggplot()+
  geom_smooth(aes(x = true_es, y = hand_calc), 
              col = 'blue', se = F)+
  geom_point(aes(x = true_es, y = hand_calc),
             col = 'blue', alpha = .3)+
  geom_smooth(aes(x = true_es, y = metafor_calc),
              col = 'red', se = F)+
  geom_point(aes(x = true_es, y = metafor_calc),
             col = 'red', alpha = .3)+
  ylab(label = 'tau')
```

I consistently make an underestimation around roughly .03 compared to metafor regardless of the size of the effect, but there is variation in the difference meaning that it is not a constant miss calculation.

Now, it appears that we have quite a bit of variation in our effect since our tau is consistently around .5. This means that if we have a true effect of 1, that effect has a standard deviation of .5, meaning that the average effect should be between .5 and 1.5 in ca 66% of observed effects. Since we have random effects, we assume that we have multiple true effects in the data and that the random estimate is the average of these true effects. The width of the distribution is what we call heterogeneity and estimate with $\tau$. My understanding is that $\tau$ and standard errors are similar but targets different levels of the analysis, standard error being the variation in means and $\tau$ being variation in true effects. $\tau$ should not be confused with the standard deviation of the observed effects, that is the variation in the sample. For example, let's generate some effects to illustrate.

```{r}
set.seed(834)
k_studies <- 10

es <- rnorm(k_studies, 1, 1)
sigma <- runif(k_studies, .3, 1)
```

We have effects and variances, the standard deviation of the effects is NOT $\tau$, it is simply the sample standard deviation. Again, $\tau$ is the standard deviation of the distribution of true effects, and this distribution of true effects is what have generated the observed sample with moment 1 being the random effect estimate and moment 2 being the heterogeneity. The following figure illustrates the point well.

![](random_effects.png){fig-align="center"}

The distribution of the true effect $\mu$ is the heterogeneity. The distribution of $\theta_k$ is the variation in the observed true effects. Tau is the standard deviation of the distribution of $\mu$, the standard error is the standard deviation of mean $\theta_k$. $\hat\theta_k$ is the estimated true effect dependent on the error between $\mu$ and $\theta_k$, and $\theta_k$ and $\hat\theta_k$.

Higgins and Thompson does a good job explaining it through the following example: They distinguish between key concepts very eloquently, stating that: we have *k* studies with true treatments effects of $\theta_i$ such that *E*\[$\theta_i$ \] = $\mu$ and var($\theta_i$ ) = $\tau^2$. From each study an estimate $y_i$ of $\theta_i$ is available such that $E[y_i|\theta_i] = \theta_i$ and  $E[y_i|\theta_i] = \sigma_i$ . The parameters underlying the scenario are $\mu, \tau^2, \sigma^2$ and *k*, where $\mu$ and $\tau^2$ are unknown and *k* and $\sigma^2$ is assumed known. 
