---
title: "thought experiment"
format: pdf
editor: visual
---

# Definitions

A true effect is the relationship between two or more valid constructs. The interaction of these constructs is what causes the effect. We have constructs A and B which make true effect C, We also have constructs X and Y which causes effect Z. How do i know which true effect the observed effects k belong to?

Could their be such a thing as construct heterogeneity and methodological heterogeneity? that might be a simple way of expressing it. If we have heterogeneity coming from the fact that the higher order constructs in the studies are different vs heterogeneity coming from the fact that the methods are different. So the constructs providing the observed effect are the same, but the method makes the effect incomparable. Perhaps design heterogeneity would be better, say we only have one construct that we are interested in, eg intelligence, and the effect we are interested in is improved intelligence from training. If we have different measures for intelligence and different training interventions we will have variation in the effects since our model is imprecise.

Maybe what i don't like about clinical heterogeneity is that they lump intervention and individuals together. I do not think they are a comparable source of heterogeneity. Individuals have some type of randomness associated with them but intervention is very systematic and design based.

# Review guidelines

Additionally, is worth mentioning that multiple other guidelines for conducting reviews exists to help address issues of heterogeneity beyond what is detailed above. A few examples are the realist review guidelines(RAMESES), Campbell review guidelines, preferred reporting items for systematic reviews and meta-analyses(PRISMA, Joanna Briggs Institute(JBI) handbook among many others that are specialized to a particular field. However the concept of substantive heterogeneity is not always directly addressed. Furthermore it is not always apparent how the concept of substantive heterogeneity should be handled even when guidelines exists. This illustrates the push and pull between the specificity of a research question and its contribution to general/theoretical knowledge. When the substantive heterogeneity is too high, the resulting synthesis will be too diffuse, but if it is too narrow, the resulting synthesis will not be interesting to anyone. This is commonly referred to as the conflict between ''lumpers'' and ''splitters''(REF). This goes to show that even when an underlying structure of conducting a review is present, individual authors will have to make judgements on the comparability of studies on a case-by-case basis, meaning that an in-depth understanding of the potential sources of substantive heterogeneity is needed.

# Between study-heterogeneity and Methodological diversity

The definition of heterogeneity in the context of meta-analysis is both very precise and very abstract. Statistical heterogeneity has a precise definition - the variation in the observed effects - but the abstract construct that is the cause of that variation, is also called heterogeneity. Different texts use different categories for the the underlying cause of statistical heterogeneity, but for our purposes it is best to view it as cause and effect, where the observed statistical heterogeneity is caused by the underlying heterogeneity in the effect. This underlying variation can be further divided in to aleatory uncertainty which is inherent to the underlying effect, and predictable epistemic uncertainty which in theory can be adjusted for to get a more precise effect estimate. Usually, it is uncertainty of the epistemic kind we are interested in accounting for when we conduct meta-regressions and subgroup analyses. If we look at the formula for a multilevel meta-regression, we can isolate these aleatory probability distributions and epistemic parameters.

$\hat\theta_k = \theta + \beta x_k + \epsilon_k + \zeta_k$

In the model above $\epsilon_k$ refers to the sampling error of the effect size from it's corresponding $\chi^2$ sampling distribution. The $\zeta_k$ term refers to the error in the mean effect sizes, meaning that we have added a level of uncertainty to our model. We no longer assume that error is attributable to random sampling from one true effect size. 

When studies have methodological differences, that is often assumed to introduce between-study heterogeneity. Meaning that because a different method is used, a different effect is measured. We assume that different effects are being measured when we an unexpected level of variation across effects, that is, when the statistical heterogeneity(variance) deviates from its expected $\chi^2$ distribution we assume that we are measuring multiple effects.
